# -*- coding: utf-8 -*-
"""Sistem_Rekomendasi_Anime.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Pzz9Sr8QIuBUX3NQV4AXf-TVckObZgS

Tujuan Sistem rekomendasi:
- Membantu pengguna menemukan anime mirip dengan anime favorit berdasarkan genre, jenis, sumber cerita, popularitas (CBF).

- Memberikan rekomendasi berdasarkan pola komunitas (CF).

# Library
"""

# Import Library yang dibutuhkan
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import ast
import random
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

"""# Load & Data Understanding"""

# Import module yang disediakan google colab untuk kebutuhan upload file
from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
# Download kaggle dataset and unzip the file
!kaggle datasets download -d vishalmane109/anime-recommendations-database
!unzip anime-recommendations-database

raw_anime = pd.read_csv("/content/Anime_data.csv")
raw_anime.head(3)

# Melihat Jumah baris dan variabel
raw_anime.shape

# Melihat nama-nama kolom dan nilai uniknya sebanyak 5
for col in raw_anime.columns:
    print(f"\nKolom: {col}")
    print(raw_anime[col].unique()[:5])

"""Berikut adalah arti dari setiap kolom secara singkat:

1. **Anime_id**: ID unik untuk setiap anime.
2. **Title**: Judul anime.
3. **Genre**: Daftar genre atau kategori anime (misal: Action, Comedy).
4. **Synopsis**: Ringkasan cerita dari anime.
5. **Type**: Jenis media anime (TV, Movie, OVA, dll).
6. **Producer**: Perusahaan yang memproduksi anime.
7. **Studio**: Studio animasi yang mengerjakan anime.
8. **Rating**: Skor rata-rata dari pengguna (biasanya dari 1â€“10).
9. **ScoredBy**: Jumlah pengguna yang memberi rating.
10. **Popularity**: Peringkat popularitas berdasarkan jumlah anggota dan aktivitas pengguna.
11. **Members**: Jumlah pengguna yang menambahkan anime ke daftar mereka (watching, completed, dll).
12. **Episodes**: Jumlah episode anime.
13. **Source**: Sumber cerita anime (Original, Manga, Game, dll).
14. **Aired**: Tanggal tayang atau rilis anime.
15. **Link**: Tautan ke halaman anime di MyAnimeList.
"""

# Meliha tipe data
raw_anime.info()

"""Dalam data tersebut terdapat 6 variabel yang bertipe data numerik dan 9 adalah object atau kategorikal"""

# Cek Missing Value
raw_anime.isnull().sum()

"""Setelah mengecek missing value:
- Hanya Variabel Anime_id, Title, dan Members yang tidak punya missing value
- Variabel Producer dan Studio mempunyai missing value yang melebihi setengahnya dari total keseluruhan data
"""

# Cek duplikasi data
raw_anime.duplicated().sum()

"""dataset tidak mempunyai data yang duplikat

# Data Preparation

Membuat dataframe baru dengan kolom-kolom utama yang akan digunakan sebagai fitur pada sistem rekomendasi anime.
"""

selected_columns = ['Title', 'Genre', 'Type', 'Rating', 'Popularity', 'Members', 'Source']
data_anime  = raw_anime[selected_columns].copy()
print(data_anime .head())

# Cek missing value
data_anime.isnull().sum()

"""Missing value pada kolom kategorikal seperti Genre, Type, dan Source diisi dengan 'Unknown' agar tetap bisa digunakan dalam perbandingan antar item. Sementara itu, missing value pada kolom numerik Rating dan Popularity diisi dengan nilai median karena lebih tahan terhadap outlier dan menjaga distribusi data tetap stabil."""

# Isi missing value pada kolom kategorikal dengan 'Unknown'
data_anime['Genre'] = data_anime['Genre'].fillna('Unknown')
data_anime['Type'] = data_anime['Type'].fillna('Unknown')
data_anime['Source'] = data_anime['Source'].fillna('Unknown')
# Isi missing value pada kolom numerik (Rating) dengan median
data_anime['Rating'] = data_anime['Rating'].fillna(data_anime['Rating'].median())
data_anime['Popularity'] = data_anime['Popularity'].fillna(data_anime['Popularity'].median())

# Cek duplikat
data_anime.duplicated().sum()

"""data_anime memlki duplikat sebanyak 78. Maka akan dilakukan penghapusan duplikat lalu reset index"""

# Hapus duplikat
data_anime = data_anime.drop_duplicates()
# reset index
data_anime = data_anime.reset_index(drop=True)

# Cek kategori dalam Type yang unik
data_anime.Type.unique()

# Cek kategori dalam Source yang unik
data_anime.Source.unique()

""""Nilai 'Other' digabung ke 'Unknown' karena keduanya menunjukkan sumber cerita yang tidak spesifik atau tidak diketahui secara jelas."

Ini dilakukan untuk menyederhanakan kategori dan mengurangi ambiguitas saat pemodelan, terutama karena keduanya tidak memberikan informasi eksplisit tentang asal cerita anime.
"""

data_anime['Source'] = data_anime['Source'].replace('Other', 'Unknown')

"""Variabel Genre berbentuk string maka akan diubah menjadi list"""

data_anime['Genre'] = data_anime['Genre'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith('[') else [])
print(data_anime['Genre'].head())

"""Variabel Genre memiliki nilai Unknown, agar tidak menjadii list kosong maka nilai unknown akan tetap disimpan"""

data_anime['Genre'] = data_anime['Genre'].apply(lambda x: ['Unknown'] if not x else x)

"""Terlihat bahwa nilai unique genre ada yg dipisahkan degan spasi dan tanda strip. Untuk menghindari gangguan pada analisis berikutnya spasi dan tanda strip akan diganti menjadi underscore"""

def preprocess_genre(genre):
    # Ganti spasi dan strip jadi underscore
    genre = genre.replace(' ', '_').replace('-', '_').lower()
    return genre

def preprocess_genres(genres_list):
    return ' '.join(preprocess_genre(g) for g in genres_list)

# apply
data_anime['Genre'] = data_anime['Genre'].apply(preprocess_genres)
print(data_anime['Genre'].head())

"""Setelah itu, akan menggabungkan variabel Type dan Source supaya fitur jadi lebih spesifik dan ngasih konteks lengkap tentang format dan asal cerita anime, misalnya TV_Original beda sama Movie_Manga"""

# Gabung kolom Type dan Source jadi satu kolom baru
data_anime['Type_Source'] = data_anime['Type'] + '_' + data_anime['Source']
# Hapus kolom Type dan Source asli setelah diproses
data_anime = data_anime.drop(columns=['Type', 'Source'])

"""### Data Preparation Untuk Collaborative Filtering (CF)

Dilakukan tambahan tahap persiapan data untuk metode collaborative filtering (CF). Metode CF berfokus pada pola interaksi pengguna terhadap anime, sehingga memerlukan data dalam bentuk relasi antara pengguna dan item (anime).
"""

# Copy data untuk proses cf
anime_cf = data_anime.copy()
anime_cf.shape

"""Dengan menggunakan metode Collaborative Filtering (CF) perlu interaksi pengguna terhadap item. Maka, Penambahan kolom user_id dilakukan karena metode Collaborative Filtering (CF) membutuhkan struktur data berbentuk interaksi antara pengguna dan item. Karena dataset yang digunakan hanya berisi informasi tentang anime tanpa mencantumkan siapa yang memberikan penilaian, maka dibuatlah kolom user_id secara mandiri"""

# Tambahkan user_id dari 00001 sampai 16924
anime_cf['user_id'] = [f"{i:05d}" for i in range(1, len(anime_cf) + 1)]

"""**Encode variabel user_id dan title**"""

# Mengubh user_id dan Title jadi List
user_ids = anime_cf['user_id'].unique().tolist()
anime_titles = anime_cf['Title'].unique().tolist()
# encode user_id dan Title
user_to_encoded = {x: i for i, x in enumerate(user_ids)}
anime_to_encoded = {x: i for i, x in enumerate(anime_titles)}
# Melakukan proses encoding angka ke ke user_id dan Title
encoded_to_user = {i: x for i, x in enumerate(user_ids)}
encoded_to_anime = {i: x for i, x in enumerate(anime_titles)}
# Tambahkan kolom encoded ke DataFrame
anime_cf['identity_user'] = anime_cf['user_id'].map(user_to_encoded)
anime_cf['anime_title'] = anime_cf['Title'].map(anime_to_encoded)

"""**Mengubah rating ke float**"""

# Ubah rating ke float
anime_cf['Rating'] = anime_cf['Rating'].values.astype(np.float32)

#  Hitung jumlah user dan anime
num_users = len(user_to_encoded)
num_anime = len(anime_to_encoded)
#  Cek rating range
min_rating = anime_cf['Rating'].min()
max_rating = anime_cf['Rating'].max()
print(f'Number of Users: {num_users}')
print(f'Number of Anime: {num_anime}')
print(f'Min Rating: {min_rating}, Max Rating: {max_rating}')

"""**Split data (80:20)**"""

# Mengacak dataset
anime_cf = anime_cf.sample(frac=1, random_state=42)
# Membuat variabel input x
x = anime_cf[['identity_user', 'anime_title']].values
# Membuat Variabel targe y dan normalisasi
y = anime_cf['Rating'].apply(lambda r: (r - min_rating) / (max_rating - min_rating)).values
# Membagi data train dan validasi
x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)

"""# Modeling  1 : Content Based Filtering (CBF)"""

# Copy data untuk CBF
anime_cbf = data_anime.copy()

"""### TF-IDF"""

# Buat vectorizer untuk Genre
tf_genre = TfidfVectorizer()
tf_genre.fit(data_anime['Genre'])
genre_features = tf_genre.get_feature_names_out()

# Buat vectorizer untuk Type_Source
tf_type_source = TfidfVectorizer()
tf_type_source.fit(data_anime['Type_Source'])
type_source_features = tf_type_source.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix_genre = tf_genre.transform(data_anime['Genre'])
tfidf_matrix_type_source = tf_type_source.transform(data_anime['Type_Source'])
# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
dense_genre = tfidf_matrix_genre.todense()
dense_type_source = tfidf_matrix_type_source.todense()

# hasil matrrix Genre terhadap title
df_tfidf_genre = pd.DataFrame(
    dense_genre,
    columns=tf_genre.get_feature_names_out(),
    index=data_anime['Title']
)
df_tfidf_genre.sample(10, axis=0).sample(15, axis=1)

"""Nilai TF-IDF itu nunjukin seberapa penting sebuah genre buat anime itu dibandingin anime lain. Kalau nilainya deket 1, berarti genre itu emang bener-bener khas buat anime itu. Kalau 0 berarti gak ada genre itu di anime tersebut. Misal, di anime Hana no Zundamaru: Junk genre parody nilainya 0,64, itu artinya genre parody cukup kuat di anime ini, tapi genre comedy cuma 0,31 yang artinya kurang dominan. Sedangkan di Cynical Hysterie Hour: Utakata no Uta, genre comedy nilainya 1, jadi comedy bener-bener genre utama buat anime ini, dan genre lain nilainya 0 alias gak ada.

### Cosine Similarity

Karena pakai dua fitur berbeda (genre & type_source) maka bisa gabungkan hasil similarity-nya
"""

# cosine similarity genre
cosine_sim_genre = cosine_similarity(tfidf_matrix_genre)
# cosine similarity type_source
cosine_sim_type_source = cosine_similarity(tfidf_matrix_type_source)
# kombinasi
cosine_sim_combined = (cosine_sim_genre + cosine_sim_type_source) / 2

"""Buat dataframe agar melihat nilai similarity antar anime berdasaarkan title"""

cosine_sim_df = pd.DataFrame(cosine_sim_combined, index=data_anime['Title'], columns=data_anime['Title'])
# Melihat similarity matrix pada setiap resto
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Nilai similarity itu nunjukin seberapa mirip dua anime berdasarkan genre dan tipe sumbernya. Nilai 0 artinya sama sekali beda, 1 artinya sama banget. Misalnya, anime Oh! My Konbu sama Sore Ike! Anpanman: Kokin-chan to Namida no Christmas nilainya 0.098, itu artinya mereka hampir gak mirip, genre dan tipe sumbernya beda jauh.

Kalau Doraemon Movie 27: Nobita no Shin Makai Daibouken - 7-nin no Mahoutsukai sama Doujouji nilainya 0.5, artinya mereka punya kemiripan sedang, mungkin ada beberapa genre atau tipe sumber yang sama.

Jadi, makin besar nilainya, makin mirip animenya, dan makin besar kemungkinan direkomendasiin bareng.

### Membuat Fungsi untuk prediksi "Sistem Rekomendasi"
"""

from difflib import get_close_matches
def anime_recommendations(name_anime, similarity_data=cosine_sim_df, items=data_anime[['Title', 'Genre', 'Type_Source']], k=5):
  # Cek apakah judul ada di kolom similarity_data
    if name_anime not in similarity_data.columns:
        # Cari judul paling mirip
        possible_titles = get_close_matches(name_anime, similarity_data.columns, n=1, cutoff=0.6)
        if not possible_titles:
            return f"Tidak ditemukan judul mirip untuk: {name_anime}"
        name_anime = possible_titles[0]
        print(f"Judul tidak ditemukan persis, pakai judul terdekat: {name_anime}")
    # Ambil nilai similarity dari anime yang dicari
    similarity_scores = similarity_data[name_anime]

    # Urutkan dari yang paling mirip, buang dirinya sendiri
    closest = similarity_scores.sort_values(ascending=False).drop(name_anime).head(k)

    # Buat dataframe rekomendasi
    closest_df = pd.DataFrame({'Title': closest.index})
    rekomendasi = closest_df.merge(items, on='Title')
    rekomendasi['Similarity'] = closest.values  # Tambahkan skor kemiripan

    return rekomendasi

hasil = anime_recommendations('Naruto', k=5)
hasil

result = anime_recommendations('Dragon Ball Kai', k=5)
result

#akkan error
#result = anime_recommendations('Shingeki no Kyojin', k=5)
#result

"""### Evaluasi CBF

# Modeling 2 : Collaborative Filtering (CF)

### Training

Membangun arsitektur model
"""

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_anime, embedding_size=128, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)

        self.user_embedding = layers.Embedding(
            input_dim=num_users,
            output_dim=embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)

        self.anime_embedding = layers.Embedding(
            input_dim=num_anime,
            output_dim=embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.anime_bias = layers.Embedding(num_anime, 1)

        # Penambahan hidden layers
        self.dense1 = layers.Dense(128, activation='relu')
        self.dense2 = layers.Dense(64, activation='relu')
        self.dense3 = layers.Dense(64, activation='relu')
        self.output_layer = layers.Dense(1, activation='sigmoid')

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        anime_vector = self.anime_embedding(inputs[:, 1])
        anime_bias = self.anime_bias(inputs[:, 1])

        dot_user_anime = tf.reduce_sum(user_vector * anime_vector, axis=1, keepdims=True)
        x = dot_user_anime + user_bias + anime_bias
        x = self.dense1(x)
        x = self.dense2(x)
        x = self.dense3(x)
        return self.output_layer(x)

"""Melatih Model"""

embedding_size = 50
model = RecommenderNet(num_users, num_anime, embedding_size) # inisialisasi model

# model compile
model.compile(
    loss='mae',
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=64,
    epochs=50,
    verbose=1,
    validation_data=(x_val, y_val)
)

"""### Sistem Rekomendas CF"""

import random
# Pilih user_id secara acak dari daftar user yang tersedia
sample_user_id = random.choice(anime_cf['user_id'].unique())
# Anime yang sudah ditonton user
watched_anime = anime_cf[anime_cf.user_id == sample_user_id]['Title'].values

# Ambil anime yang belum ditonton
not_watched = anime_cf[~anime_cf['Title'].isin(watched_anime)]['Title'].unique()

# Filter hanya anime yang ada dalam encoded mapping
not_watched = list(set(not_watched).intersection(set(anime_to_encoded.keys())))

# Encode anime yang belum ditonton
not_watched_encoded = [[anime_to_encoded[x]] for x in not_watched]
# Encode user_id ke bentuk numerik (identity_user) agar cocok untuk model
user_encoded = user_to_encoded[sample_user_id]

# Buat array prediksi
user_anime_array = np.hstack(
    ([[user_encoded]] * len(not_watched_encoded), not_watched_encoded)
)

# Prediksi skor
ratings = model.predict(user_anime_array).flatten()

# Urutkan dari skor tertinggi
top_indices = ratings.argsort()[::-1]
# Ambil anime yang direkomendasikan (top 10)
top_anime_encoded = [not_watched_encoded[i][0] for i in top_indices[:10]]
recommended_titles = [encoded_to_anime[x] for x in top_anime_encoded]
predicted_ratings = [ratings[i] for i in top_indices[:10]]

# Ambil data tambahan (rating, popularity, members)
rekomendasi_df = pd.DataFrame({
    'Title': recommended_titles,
    'Predicted_Rating': predicted_ratings
})

# Gabungkan dengan info dari anime_cf (drop duplikat biar merge akurat)
anime_info = anime_cf.drop_duplicates(subset='Title')[['Title', 'Rating', 'Popularity',  "Genre",'Members']]
rekomendasi_df = rekomendasi_df.merge(anime_info, on='Title', how='left')

# Tampilkan
print(f"\n Rekomendasi anime untuk user {sample_user_id}:\n")
for i, row in rekomendasi_df.iterrows():
    print(f"{i+1}. {row['Title']}")
    print(f"   Predicted Rating: {row['Predicted_Rating']:.2f}")
    print(f"   Original Rating: {row['Rating']}")
    print(f"   Popularity: {int(row['Popularity'])}")
    print(f"   Genre: {row['Genre']}")
    print(f"   Members: {int(row['Members']) if not pd.isna(row['Members']) else 0}")
    print()

"""# EVALUASI

### a. precision@k untuk CBF
"""

def precision_at_k(name_anime, k=5):
    # Dapatkan genre anime target
    target_genre = data_anime.loc[data_anime['Title'] == name_anime, 'Genre'].values[0]

    # Dapatkan rekomendasi anime (misal pakai fungsi yang sudah ada)
    rekomendasi = anime_recommendations(name_anime, k=k)

    # Hitung berapa yang genre-nya sama dengan target
    relevan = rekomendasi['Genre'].apply(lambda x: target_genre in x)

    # Precision@k = jumlah relevan / k
    precision = relevan.sum() / k

    return precision

hasil_precision = precision_at_k('Naruto', k=5)
print(f"Precision@5 untuk Naruto: {hasil_precision:.2f}")

"""Sebagai contoh, nilai Precision@5 untuk anime "Naruto" adalah 0.20. Artinya, dari 5 rekomendasi teratas yang diberikan, hanya sekitar 20% (1 dari 5) yang memiliki genre yang sama dengan "Naruto". Hasil ini menunjukkan bahwa akurasi rekomendasi berdasarkan genre masih dapat ditingkatkan agar lebih relevan dengan preferensi pengguna.

B. Loss Curve untuk CF
"""

plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title("Loss Curve")
plt.show()

"""Berdasarkan Kurva Loss:

- Train Loss menurun dan stabil di angka rendah seiring bertambahnya epoch.
- Val Loss Setelah beberapa epoch awal, tidak menurun, justru tetap tinggi dan cenderung stagnan bahkan sedikit naik dengan fluktuasi.
- Gap besar antara train loss dan val loss dan Val loss tidak membaik
- Maka, Model mengalami Overfitting
"""

